{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4265b4-0769-465b-8990-63fd6539f77d",
   "metadata": {},
   "source": [
    "# Отчет по домашней работе №4: Вопросно-ответные системы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ea24e-a7d8-4130-9ca8-6ba8c4ebee68",
   "metadata": {},
   "source": [
    "## Импорты и подпрограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a20bbb-b8e2-44f1-89f1-c12e241d35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "\n",
    "PATH_TRAIN = \"../data/train.jsonl\"\n",
    "PATH_TEST = \"../data/dev.jsonl\"\n",
    "\n",
    "\n",
    "def log(msg:str, headers=None):\n",
    "    dttm = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    if (\n",
    "        (\"__main__\" in globals() or \"__main__\" in locals())\n",
    "        and hasattr(__main__, \"__file__\")\n",
    "    ):\n",
    "        script = os.path.basename(__main__.__file__)\n",
    "    else:\n",
    "        script = \"jupyter\"\n",
    "    if headers is None:\n",
    "        headers = []\n",
    "    header_line = f\"[{dttm}][{script}]\" + \"\".join(f\"[{h}]\" for h in headers)\n",
    "    print(f\"{header_line} {msg}\")\n",
    "\n",
    "\n",
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    records = []\n",
    "    with open(path) as fp:\n",
    "        for line in fp:\n",
    "            record = json.loads(line.strip())\n",
    "            records.append(record)\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare(df, batch_size=128):\n",
    "    df[\"tp\"] = df.question + \". \" + df.passage\n",
    "    texts = df.tp.tolist()\n",
    "    batches = batched(texts, batch_size)\n",
    "    return batches\n",
    "    \n",
    "\n",
    "def make_bert_embs(batches):\n",
    "    b_vecs = []\n",
    "    for idx, batch in enumerate(batches):\n",
    "        if idx%5==0:\n",
    "            log(f\"{idx} batches complete\", [\"make_embs\"])\n",
    "        tokens = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            vecs = (\n",
    "                model(**tokens)\n",
    "                .last_hidden_state\n",
    "                .mean(1)\n",
    "                .detach()\n",
    "                .numpy()\n",
    "            )\n",
    "        b_vecs.append(vecs)\n",
    "    return np.vstack(b_vecs)\n",
    "    \n",
    "\n",
    "def get_train_bert_embs():\n",
    "    return get_bert_embs(\"train.npy\", PATH_TRAIN)\n",
    "\n",
    "\n",
    "def get_test_bert_embs():\n",
    "    return get_bert_embs(\"test.npy\", PATH_TEST)\n",
    "\n",
    "\n",
    "def get_bert_embs(filename, path_in, cache_dir=\"./\"):\n",
    "    fpath = os.path.join(cache_dir, filename)\n",
    "    if os.path.exists(fpath):\n",
    "        with open(fpath, 'rb') as fp:\n",
    "            embs = np.load(fp)\n",
    "    else:\n",
    "        batches = prepare(read_data(path_in))  # 590\n",
    "        embs = make_bert_embs(batches)\n",
    "        with open(fpath, 'wb') as fp:\n",
    "            np.save(fp, embs)\n",
    "    return embs\n",
    "\n",
    "\n",
    "def get_scores(y_true, y_score, thr=0.5):\n",
    "    y_pred = (thr<y_score).astype(int)\n",
    "    auc = roc_auc_score(y_score=y_score, y_true=y_true)\n",
    "    f1 = f1_score(y_true=y_true, y_pred=y_pred)\n",
    "    prec = precision_score(y_true=y_true, y_pred=y_pred)\n",
    "    rec = recall_score(y_true=y_true, y_pred=y_pred)\n",
    "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    return {\n",
    "        \"auroc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"accuracy\": acc\n",
    "    }\n",
    "\n",
    "\n",
    "def print_scores(msg, rec):\n",
    "    scores = \", \".join(\n",
    "        f\"{k}={v}\" for k,v in rec.items()\n",
    "    )\n",
    "    print(f\"{msg}: {scores}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8ddbf-db06-41f7-a93c-172a15863880",
   "metadata": {},
   "source": [
    "## Часть 1. [1 балл] Эксплоративный анализ\n",
    "1. Посчитайте долю yes и no классов в корпусе\n",
    "2. Оцените среднюю длину вопроса\n",
    "3. Оцените среднюю длину параграфа\n",
    "4. Предположите, по каким эвристикам были собраны вопросы (или найдите ответ в статье). Продемонстриуйте, как эти эвристики повлияли на структуру корпуса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d08314-ce4b-41d4-878e-cd7228698fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = read_data(PATH_TRAIN)\n",
    "d_test = read_data(PATH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090be01-2aa4-4e92-a3b9-54be4b125f05",
   "metadata": {},
   "source": [
    "### 1.1 Оценка средней доли YES в корпусе\n",
    "Доля NO это 1-YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842f99fa-06b4-4580-9625-820b05ec9d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scope</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.623104</td>\n",
       "      <td>0.484634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>0.621713</td>\n",
       "      <td>0.485034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scope      mean       std\n",
       "0  train  0.623104  0.484634\n",
       "1   test  0.621713  0.485034"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"scope\": \"train\", \n",
    "            \"mean\": d_train.answer.astype(int).mean(),\n",
    "            \"std\": d_train.answer.astype(int).std(),\n",
    "        },\n",
    "        {\n",
    "            \"scope\": \"test\", \n",
    "            \"mean\": d_test.answer.astype(int).mean(),\n",
    "            \"std\": d_test.answer.astype(int).std(),\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566cc0d-181c-4df4-a991-3dd359e92a24",
   "metadata": {},
   "source": [
    "### 1.2 Оценка средней длины вопроса\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "103005d3-7c3c-4b6b-a555-d3d8d08b9ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scope</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>43.991938</td>\n",
       "      <td>8.854335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>43.206422</td>\n",
       "      <td>7.785706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scope       mean       std\n",
       "0  train  43.991938  8.854335\n",
       "1   test  43.206422  7.785706"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"scope\": \"train\", \n",
    "            \"mean\": d_train.question.str.len().mean(),\n",
    "            \"std\": d_train.question.str.len().std(),\n",
    "        },\n",
    "        {\n",
    "            \"scope\": \"test\", \n",
    "            \"mean\": d_test.question.str.len().mean(),\n",
    "            \"std\": d_test.question.str.len().std(),\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a4849-b3a7-49ab-941e-5e15d59d79a2",
   "metadata": {},
   "source": [
    "### 1.3 Оценка средней длины параграфа\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11c3818-6ca3-4cf0-a13c-59c1ea15a0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scope</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>565.613026</td>\n",
       "      <td>323.137498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>559.052294</td>\n",
       "      <td>328.796047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scope        mean         std\n",
       "0  train  565.613026  323.137498\n",
       "1   test  559.052294  328.796047"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"scope\": \"train\", \n",
    "            \"mean\": d_train.passage.str.len().mean(),\n",
    "            \"std\": d_train.passage.str.len().std(),\n",
    "        },\n",
    "        {\n",
    "            \"scope\": \"test\", \n",
    "            \"mean\": d_test.passage.str.len().mean(),\n",
    "            \"std\": d_test.passage.str.len().std(),\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9e0e9-3fc4-4a59-a7ac-f7ff260aa2d6",
   "metadata": {},
   "source": [
    "### 1.4 Структура корпуса\n",
    "Датасет собирался на основе датасета Natural Questions Corpus, который состоит из пар <Поисковый запрос, текст с ответом>. \n",
    "\n",
    "Датасет имеетследующие особенности:\n",
    "- Выбирались такие пары, где запрос начинался на какое-либо из следующих слов: “did”, “do”, “does”, “is”, “are”, “was”, “were”, “have”, “has”, “can”, “could”, “will”, “would”.\n",
    "- Также отсекалсь слишом короткие вопросы.\n",
    "- В датасете ответ на запрос всегда можно локализовать в тексте. То есть нет вопросов, требующих вывод по тексту вместо поиска по тексту.\n",
    "- Аннотаторы из-всего текста выбирали конкретный параграф, в котором содержался ответ. Поэтому текст всегда представляет собой один параграф.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c980e89-4708-4740-9368-3318fcabd775",
   "metadata": {},
   "source": [
    "## Часть 2. [1 балл] Baseline\n",
    "1. Оцените accuracy точность совсем простого базового решения: присвоить каждой паре вопрос-ответ в dev части самый частый класс из train части\n",
    "2. Оцените accuracy чуть более сложного базового решения: fasttext на текстах, состоящих из склееных вопросов и абзацев (' '.join([question, passage]))\n",
    "\n",
    "Почему fasttext плохо справляется с этой задачей?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71983303-f29d-4f4b-8883-a623b01ffcd5",
   "metadata": {},
   "source": [
    "# Бейзлайн 1: Наиболее частый ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f89c721-b29b-4a2d-917b-4f2fc778fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = read_data(PATH_TRAIN)\n",
    "d_test = read_data(PATH_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b0c8fe5-6b1a-4732-8539-4536960c6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее частый ответ: ans_train=True\n",
      "Метрики Бейзлайн-1 на обучающей выборке {'auroc': 0.5, 'f1': 0.7677929547088426, 'precision': 0.6231038506417736, 'recall': 1.0, 'accuracy': 0.6231038506417736}\n",
      "Метрики Бейзлайн-1 на тестовой выборке {'auroc': 0.5, 'f1': 0.7667358099189139, 'precision': 0.6217125382262997, 'recall': 1.0, 'accuracy': 0.6217125382262997}\n"
     ]
    }
   ],
   "source": [
    "ans_train = d_train.answer.mode().values.item()\n",
    "# ans_test = d_test.answer.mode().values.item()\n",
    "p_train = np.array([ans_train for i in range(d_train.shape[0])])\n",
    "p_test = np.array([ans_train for i in range(d_test.shape[0])])\n",
    "print(f\"Наиболее частый ответ: {ans_train=}\")\n",
    "bl1_train = get_scores(y_true=d_train.answer.tolist(), y_score=p_train)\n",
    "bl1_test = get_scores(y_true=d_test.answer.tolist(), y_score=p_test)\n",
    "print(\"Метрики Бейзлайн-1 на обучающей выборке\", bl1_train)\n",
    "print(\"Метрики Бейзлайн-1 на тестовой выборке\", bl1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1aa20c-7168-4a8c-9b02-7825b2d160a4",
   "metadata": {},
   "source": [
    "# Бейзлайн 2: FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebceabe9-59a6-443a-b0b3-37002a2b8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5874, number of negative: 3553\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 9427, number of used features: 300\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623104 -> initscore=0.502744\n",
      "[LightGBM] [Info] Start training from score 0.502744\n",
      "Метрики Бейзлайн-2(FastText) на обучающей выборке:  {'auroc': 0.9948531939277219, 'f1': 0.9497360941940722, 'precision': 0.9079335506908865, 'recall': 0.9955737146748382, 'accuracy': 0.9343375411053357}\n",
      "Метрики Бейзлайн-2(FastText) на тестовой выборке:  {'auroc': 0.6582806490004656, 'f1': 0.7660297239915074, 'precision': 0.673888681359731, 'recall': 0.8873585833743236, 'accuracy': 0.6629969418960244}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.fasttext.load_facebook_vectors(\"../data/cc.ru.300.bin\")\n",
    "d_train[\"tp\"] = d_train.question + \". \" + d_train.passage\n",
    "d_test[\"tp\"] = d_test.question + \". \" + d_test.passage\n",
    "tokens_train = d_train.tp.apply(lambda line: [x for x in gensim.utils.tokenize(line)]).tolist()\n",
    "tokens_test = d_test.tp.apply(lambda line: [x for x in gensim.utils.tokenize(line)]).tolist()\n",
    "x_train = np.array(\n",
    "    [model.get_mean_vector(tokens).tolist() for tokens in tokens_train]\n",
    ")\n",
    "x_test =  np.array(\n",
    "    [model.get_mean_vector(tokens).tolist() for tokens in tokens_test]\n",
    ")\n",
    "# params = dict(min_child_samples=10) # 0.7706774951912803\n",
    "params = dict(min_child_samples=8) # 0.7706774951912803\n",
    "est = lgb.LGBMClassifier(\n",
    "    **params\n",
    ")\n",
    "est.fit(x_train, d_train.answer.values)\n",
    "bl2_train = get_scores(\n",
    "    y_true=d_train.answer.values,\n",
    "    y_score=est.predict_proba(x_train)[:,1]\n",
    ")\n",
    "print(\"Метрики Бейзлайн-2(FastText) на обучающей выборке: \", bl2_train)\n",
    "bl2_test = get_scores(\n",
    "    y_true=d_test.answer.values,\n",
    "    y_score=est.predict_proba(x_test)[:,1]\n",
    ")\n",
    "print(\"Метрики Бейзлайн-2(FastText) на тестовой выборке: \", bl2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5bbb8-b484-4439-86fc-76449beb34fd",
   "metadata": {},
   "source": [
    "# Часть 3. [1 балл] Используем эмбеддинги предложений\n",
    "\n",
    "1. Постройте BERT эмбеддинги вопроса и абзаца. Обучите логистическую регрессию на конкатенированных эмбеддингах вопроса и абзаца и оцените accuracy этого решения. \n",
    "\n",
    "[bonus] Используйте другие модели эмбеддингов, доступные, например, в библиотеке 🤗 Transformers. Какая модель эмбеддингов даст лучшие результаты?\n",
    "\n",
    "[bonus] Предложите метод аугментации данных и продемонстрируйте его эффективность. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13340b0f-0680-447a-a181-af3aea9119c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5874, number of negative: 3553\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 195840\n",
      "[LightGBM] [Info] Number of data points in the train set: 9427, number of used features: 768\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623104 -> initscore=0.502744\n",
      "[LightGBM] [Info] Start training from score 0.502744\n",
      "Метрики с Bert-эмбеддингами на обучающей выборке: {'auroc': 0.9957578517475676, 'f1': 0.9628714909031036, 'precision': 0.9322493224932249, 'recall': 0.9955737146748382, 'accuracy': 0.9521586931155193}\n",
      "Метрики с Bert-эмбеддингами на тестовой выборке: {'auroc': 0.6787914527515079, 'f1': 0.767217335335765, 'precision': 0.680365296803653, 'recall': 0.8794884407279882, 'accuracy': 0.6681957186544343}\n"
     ]
    }
   ],
   "source": [
    "# считаем ruBert-эмбеддинги или загружаем из кэша\n",
    "x_train = get_train_bert_embs()\n",
    "x_test = get_test_bert_embs()\n",
    "d_train = read_data(PATH_TRAIN)\n",
    "d_test = read_data(PATH_TEST)\n",
    "y_train = d_train.answer.astype(int)\n",
    "y_test = d_test.answer.astype(int)\n",
    "\n",
    "\n",
    "# params = dict(min_child_samples=8) # 0.669559980610946\n",
    "params = dict(min_child_samples=100) # 0.669559980610946\n",
    "\n",
    "est = lgb.LGBMClassifier(**params)\n",
    "est.fit(x_train, d_train.answer.values)\n",
    "\n",
    "bert_train = get_scores(\n",
    "    y_true=d_train.answer.values,\n",
    "    y_score=est.predict_proba(x_train)[:,1]\n",
    ")\n",
    "bert_test = get_scores(\n",
    "    y_true=d_test.answer.values,\n",
    "    y_score=est.predict_proba(x_test)[:,1]\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Метрики с Bert-эмбеддингами на обучающей выборке: {bert_train}\")\n",
    "print(f\"Метрики с Bert-эмбеддингами на тестовой выборке: {bert_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a5b5c-a6e6-4c05-9218-12a7871fe8f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Часть 4. [3 балла] DrQA-подобная архитектура\n",
    "<h1>...</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f299448-19e9-4529-a3f9-2df6f30f5b4e",
   "metadata": {},
   "source": [
    "# Часть 5. [3 балла] BiDAF-подобная архитектура\n",
    "<h1>...</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d279caa-f0fe-4c20-95f7-c17fcfda3cf4",
   "metadata": {},
   "source": [
    "# Часть 6. [1 балл] Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a627de-4e39-4ff4-8a15-10bee5c0d003",
   "metadata": {},
   "source": [
    "## 6.1. Сравнение результатов\n",
    "Были рассмотрены такие вариаенты как: \n",
    "- Наиболее частый ответ (Бейзлайн-1)\n",
    "- XGB + FastText-эмбеддинги (Бейзлайн-2)\n",
    "- XGB + Bert-эмбеддинги\n",
    "\n",
    "Для сравнения по f1, precision, recall и accuracy для простоты использовался порог равный 0.5, можно попробовать выбирать наилучший порог: максимизировать f1.\n",
    "\n",
    "Сравним результаты обучения моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c62a2f89-fbbc-43e8-913b-d571da6d6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_scores = pd.DataFrame(\n",
    "    [\n",
    "        {'model': 'bl1_mode', 'scope': 'train', 'auroc': 0.5, 'f1': 0.7677929547088426, 'precision': 0.6231038506417736, 'recall': 1.0, 'accuracy': 0.6231038506417736},\n",
    "        {'model': 'bl1_mode', 'scope': 'test', 'auroc': 0.5, 'f1': 0.7667358099189139, 'precision': 0.6217125382262997, 'recall': 1.0, 'accuracy': 0.6217125382262997},\n",
    "        \n",
    "        {'model': 'fasttext', 'scope': 'train', 'auroc': 0.9948531939277219, 'f1': 0.9497360941940722, 'precision': 0.9079335506908865, 'recall': 0.9955737146748382, 'accuracy': 0.9343375411053357},\n",
    "        {'model': 'fasttext', 'scope': 'test', 'auroc': 0.6582806490004656, 'f1': 0.7660297239915074, 'precision': 0.673888681359731, 'recall': 0.8873585833743236, 'accuracy': 0.6629969418960244},\n",
    "        \n",
    "        {'model': 'bert', 'scope': 'train', 'auroc': 0.9976606494140341, 'f1': 0.966922378949105, 'precision': 0.9379100656104977, 'recall': 0.9977868573374191, 'accuracy': 0.9574626074042644},\n",
    "        {'model': 'bert', 'scope': 'test', 'auroc': 0.6787914527515079, 'f1': 0.767217335335765, 'precision': 0.680365296803653, 'recall': 0.8794884407279882, 'accuracy': 0.6681957186544343}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0b074fe-e666-4a13-9d77-6cf73670a41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>scope</th>\n",
       "      <th>auroc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bl1_mode</td>\n",
       "      <td>train</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.767793</td>\n",
       "      <td>0.623104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.623104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fasttext</td>\n",
       "      <td>train</td>\n",
       "      <td>0.994853</td>\n",
       "      <td>0.949736</td>\n",
       "      <td>0.907934</td>\n",
       "      <td>0.995574</td>\n",
       "      <td>0.934338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bert</td>\n",
       "      <td>train</td>\n",
       "      <td>0.997661</td>\n",
       "      <td>0.966922</td>\n",
       "      <td>0.937910</td>\n",
       "      <td>0.997787</td>\n",
       "      <td>0.957463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  scope     auroc        f1  precision    recall  accuracy\n",
       "0  bl1_mode  train  0.500000  0.767793   0.623104  1.000000  0.623104\n",
       "2  fasttext  train  0.994853  0.949736   0.907934  0.995574  0.934338\n",
       "4      bert  train  0.997661  0.966922   0.937910  0.997787  0.957463"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_scores[d_scores.scope==\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "282cd8f1-38db-470d-a5d2-f026833bb312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>scope</th>\n",
       "      <th>auroc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bl1_mode</td>\n",
       "      <td>test</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.766736</td>\n",
       "      <td>0.621713</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.621713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fasttext</td>\n",
       "      <td>test</td>\n",
       "      <td>0.658281</td>\n",
       "      <td>0.766030</td>\n",
       "      <td>0.673889</td>\n",
       "      <td>0.887359</td>\n",
       "      <td>0.662997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bert</td>\n",
       "      <td>test</td>\n",
       "      <td>0.678791</td>\n",
       "      <td>0.767217</td>\n",
       "      <td>0.680365</td>\n",
       "      <td>0.879488</td>\n",
       "      <td>0.668196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model scope     auroc        f1  precision    recall  accuracy\n",
       "1  bl1_mode  test  0.500000  0.766736   0.621713  1.000000  0.621713\n",
       "3  fasttext  test  0.658281  0.766030   0.673889  0.887359  0.662997\n",
       "5      bert  test  0.678791  0.767217   0.680365  0.879488  0.668196"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_scores[d_scores.scope==\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc22f9-db0f-4546-bad8-85bc10d74a79",
   "metadata": {},
   "source": [
    "Наилучшие результаты по ROC AUC, F1, Accuracy у модели Bert+XGB. Затем модель FastText+XGB превосходит первый бейзлайн по ROC AUC и Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34758d-9f99-494c-9ab5-aa578c7b121c",
   "metadata": {},
   "source": [
    "# 6.2 Резюме о проделанной работе\n",
    "Были выполнены такие модели как:\n",
    "- Наиболее частый ответ (Бейзлайн-1)\n",
    "- XGB + FastText-эмбеддинги (Бейзлайн-2)\n",
    "- XGB + Bert-эмбеддинги\n",
    "\n",
    "Не выполены модели:\n",
    "- DrQA\n",
    "- BiDAF\n",
    "\n",
    "Надеюсь успеть сделать хотя бы одну из этих моделей. Выполнению моделей пока помешало что нет готовых имплементаций для BoolQ  моделей DrQA и BiDAF, надо переделывать модели под задание. Этого мне не хватает. А помогла библиотека HuggngFace - очень удобная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a4f1a-d2ef-4cbe-85a5-ae9aed03c89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
