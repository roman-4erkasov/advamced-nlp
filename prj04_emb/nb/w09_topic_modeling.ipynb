{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30380e5-f13c-4d8e-8093-e8b28f55ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import log\n",
    "\n",
    "def get_date(url):\n",
    "    dates = re.findall(r\"\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d\", url)\n",
    "    return next(iter(dates), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd00546-2146-480e-8ad5-d0ca8b568886",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/prepared.pkl\", \"rb\") as fp:\n",
    "    prepared = pkl.load(fp)\n",
    "vocabulary = prepared[\"vocabulary\"]\n",
    "texts = prepared[\"texts\"]\n",
    "contexts = prepared[\"contexts\"]\n",
    "test_texts = prepared[\"test_texts\"]\n",
    "y_train = prepared[\"y_train\"]\n",
    "y_test = prepared[\"y_test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c31046-4f9e-41b0-9f50-a60bb8cd1a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = prepared[\"texts_train\"]\n",
    "text_test = prepared[\"texts_test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6cfbd0-e19f-4737-af37-6d8c186b9622",
   "metadata": {},
   "source": [
    "# CBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe6ff2-8212-42cf-a479-2b7f4f84918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from types import NoneType\n",
    "from razdel import tokenize\n",
    "from typing import Union, Mapping, Any\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "\n",
    "class CBOWModel(LightningModule):\n",
    "    def __init__(self, vocab_size=71186, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.embeddings = th.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = th.nn.Linear(embedding_dim, vocab_size)\n",
    "        self.loss = th.nn.CrossEntropyLoss()\n",
    "        self.train_outputs = []\n",
    "        self.val_outputs = []\n",
    "        self.test_outputs = []\n",
    "    \n",
    "    def forward(self, centrals, contexts):\n",
    "        projections = self.embeddings.forward(contexts).sum(axis=1)\n",
    "        logits = self.out_layer.forward(projections)\n",
    "        loss = self.loss(logits, centrals.squeeze())\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        result = self(*batch)\n",
    "        self.log(\"loss\", result)\n",
    "        return {'loss': result}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        result = self(*batch)\n",
    "        self.log(\"val_loss\", result)  \n",
    "        return {'val_loss': result}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        result = self(*batch)\n",
    "        self.log(\"test_loss\", result)\n",
    "        return {'test_loss': self(*batch)}\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self,\n",
    "        outputs: Union[th.Tensor, Mapping[str, Any], NoneType],\n",
    "        batch: Any,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int = 0,\n",
    "    ) -> None:\n",
    "        self.train_outputs.append(outputs)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = self.train_outputs\n",
    "        avg_loss = th.stack([x['loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'loss': avg_loss}\n",
    "        self.log(\"train_loss_epoch\", avg_loss, on_step=False, on_epoch=True)\n",
    "        return {'train_loss_epoch': avg_loss, 'progress_bar': tensorboard_logs}\n",
    "    \n",
    "    def on_validation_batch_end(\n",
    "        self,\n",
    "        outputs: Union[th.Tensor, Mapping[str, Any], NoneType],\n",
    "        batch: Any,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int = 0,\n",
    "    ) -> None:\n",
    "        self.val_outputs.append(outputs)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.val_outputs\n",
    "        avg_loss = th.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        self.log(\"val_loss_epoch\", avg_loss, on_step=False, on_epoch=True)\n",
    "        return {'val_loss_epoch': avg_loss, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def on_test_batch_end(\n",
    "        self,\n",
    "        outputs: Union[th.Tensor, Mapping[str, Any], NoneType],\n",
    "        batch: Any,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int = 0,\n",
    "    ) -> None:\n",
    "        self.test_outputs.append(outputs)\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        outputs = self.test_outputs \n",
    "        avg_loss = th.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.log(\"test_loss_epoch\", avg_loss, on_step=False, on_epoch=True)\n",
    "        return {'test_loss_epoch': avg_loss, 'progress_bar': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = th.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return [optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bf15d-f527-4b46-9d34-1f1829c1fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_by_text(embeddings, vocabulary, phrase):\n",
    "    embeddings = np.array(\n",
    "        [\n",
    "            embeddings[vocabulary.get_index(word.text.lower())] \n",
    "            for word in tokenize(phrase)\n",
    "        ]\n",
    "    )\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def get_emb_by_tokens(embeddings, vocabulary, tokens):\n",
    "    vectors = []\n",
    "    for tok in tokens:\n",
    "        idx = vocabulary.get_index(tok)\n",
    "        vec = embeddings[idx,:] \n",
    "        vectors.append(vec)\n",
    "    return np.mean(np.array(vectors), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d97016-ed50-489f-b64f-d7fcca53f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODEL = \"ckpt/w06-epoch=9-val_loss=-98685.14.ckpt\"\n",
    "model = CBOWModel.load_from_checkpoint(PATH_MODEL)\n",
    "embeddings = model.embeddings.weight.cpu().data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d480ea-0878-4326-bd77-daa037a98124",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(text_train), embeddings.shape[1]))\n",
    "for i, text in enumerate(text_train):\n",
    "    X_train[i, :] = get_emb_by_tokens(embeddings, vocabulary, text)\n",
    "\n",
    "X_test = np.zeros((len(text_test), embeddings.shape[1]))\n",
    "for i, text in enumerate(text_test):\n",
    "    X_test[i, :] = get_emb_by_text(embeddings, vocabulary, text)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ae2c3-2457-4fef-b980-bbffadc9dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49068bff-64b7-43e3-a5f4-0423a690e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446742d-86fd-4912-8fa0-02bda4043a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "est = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    subsample=0.6,\n",
    "    max_depth=2,\n",
    "    min_child_samples=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac3f04-84c7-4c07-800d-613bb7baeca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16e934-6dce-4413-8f75-8a63cdc69bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06fa33d-61b9-4162-9b2f-eb6348c6f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = est.predict_proba(X_train)\n",
    "roc_auc_score(y_true=y_train, y_score=p_train, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f47b41-86de-4597-b988-66f82f6ae821",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = est.predict_proba(X_test)\n",
    "roc_auc_score(y_true=y_test, y_score=p_test, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31144a1b-dfe2-4236-a803-e6d5e5a551a2",
   "metadata": {},
   "source": [
    "# CBoW + NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea50239-7008-41c5-aa88-b2c243bf413e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4dc7d1-f77d-4262-a3d6-5e9924c199ba",
   "metadata": {},
   "source": [
    "# XLM-RoBerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ca08a7-697b-4b93-b954-dfaead4b7b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatuus/advanced-nlp/prj04_emb/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\").eval()\n",
    "\n",
    "# prepare input\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8f8a7-2a32-4ec1-a744-a974f7a191dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "for i, text in enumerate(text_train):\n",
    "    tokens = tokenizer(text,return_tensors='pt', max_length=512)\n",
    "    out = model(**tokens, output_hidden_states=True)\n",
    "    X_train[i, :] = out.hidden_states[-1].squeeze().mean(0).detach()\n",
    "    if i % 100 == 0:\n",
    "        log(f\"{i=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2def1bb-5aed-43b0-8be4-6f9b71a44613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94cafc1a-3fa5-4f04-989a-144dc4845025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) â†’ ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc70b2f-1f66-47fe-9c6b-fa0729e7d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batched(text_train, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80af6d1-c7fc-4d90-9f48-27ca82b136b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches = len([_ for b in batches])\n",
    "n_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7d866-0620-4c69-81c2-cfbac1c4dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-07 02:32:38.540252][jupyter] i=0\n",
      "[2024-04-07 02:33:10.643498][jupyter] i=1\n",
      "[2024-04-07 02:33:41.743247][jupyter] i=2\n",
      "[2024-04-07 02:34:12.703552][jupyter] i=3\n",
      "[2024-04-07 02:34:43.927627][jupyter] i=4\n",
      "[2024-04-07 02:35:14.883332][jupyter] i=5\n",
      "[2024-04-07 02:35:45.899380][jupyter] i=6\n",
      "[2024-04-07 02:36:17.016473][jupyter] i=7\n",
      "[2024-04-07 02:36:47.931583][jupyter] i=8\n",
      "[2024-04-07 02:37:18.934758][jupyter] i=9\n",
      "[2024-04-07 02:37:50.027331][jupyter] i=10\n",
      "[2024-04-07 02:38:22.188971][jupyter] i=11\n",
      "[2024-04-07 02:38:53.372067][jupyter] i=12\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "batches = batched(text_train, 128)\n",
    "for i, batch in enumerate(batches):\n",
    "    tokens = tokenizer(\n",
    "        list(batch),\n",
    "        return_tensors='pt', \n",
    "        max_length=512, \n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )    \n",
    "    out = model(**tokens, output_hidden_states=True)\n",
    "    out_batch = out.hidden_states[-1].squeeze().mean(0).detach().numpy()\n",
    "    outputs.append(out_batch)\n",
    "    del batch, tokens, out, out_batch\n",
    "    # if i % 100 == 0:\n",
    "    log(f\"{i=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eee05-4fa8-485c-9b9a-572a7b9deb32",
   "metadata": {},
   "source": [
    "[2024-04-07 02:06:20.448628][jupyter] i=0\n",
    "[2024-04-07 02:06:35.924847][jupyter] i=1\n",
    "[2024-04-07 02:06:51.723109][jupyter] i=2\n",
    "[2024-04-07 02:07:07.197572][jupyter] i=3\n",
    "[2024-04-07 02:07:22.617140][jupyter] i=4\n",
    "[2024-04-07 02:07:38.066255][jupyter] i=5\n",
    "[2024-04-07 02:07:53.547985][jupyter] i=6\n",
    "[2024-04-07 02:08:08.893030][jupyter] i=7\n",
    "[2024-04-07 02:08:24.327267][jupyter] i=8\n",
    "[2024-04-07 02:08:39.694576][jupyter] i=9\n",
    "[2024-04-07 02:08:55.101007][jupyter] i=10\n",
    "[2024-04-07 02:09:10.433617][jupyter] i=11\n",
    "[2024-04-07 02:09:25.801040][jupyter] i=12\n",
    "[2024-04-07 02:09:41.433941][jupyter] i=13\n",
    "[2024-04-07 02:09:56.692024][jupyter] i=14\n",
    "[2024-04-07 02:10:11.953853][jupyter] i=15\n",
    "[2024-04-07 02:10:27.398088][jupyter] i=16\n",
    "[2024-04-07 02:10:42.791378][jupyter] i=17\n",
    "[2024-04-07 02:10:58.184608][jupyter] i=18\n",
    "[2024-04-07 02:11:13.675452][jupyter] i=19\n",
    "[2024-04-07 02:11:28.993994][jupyter] i=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dcabcc4-b159-4b68-8076-3e380d7a908d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m batch, tokens\u001b[38;5;66;03m#, out, out_batch \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "# del batch, tokens, out, out_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb557f9b-15c5-46fe-9f2b-4e01f7b0bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((len(text_test), 768))\n",
    "for i, text in enumerate(text_test):\n",
    "    tokens = tokenizer(batch,return_tensors='pt', max_length=512)\n",
    "    out = model(**tokens, output_hidden_states=True)\n",
    "    X_test[i, :] = out.hidden_states[-1].mean(1).detach().numpy()\n",
    "    if i % 100 == 0:\n",
    "        log(f\"{i=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46138493-6a41-4531-bf9c-c45974bc1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085a104-bbbf-452d-bb9e-d6881af636b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = est.predict_proba(X_train)\n",
    "roc_auc_score(y_true=y_train, y_score=p_train, multi_class=\"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b614e3-bc9b-4f41-adef-f1fc0704da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = est.predict_proba(X_test)\n",
    "roc_auc_score(y_true=y_test, y_score=p_test, multi_class=\"ovo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
