{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e2edee-869b-41cb-baaf-85b8e3230000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatuus/advanced-nlp/prj04_emb/vegpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from utils import log\n",
    "from itertools import islice\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "def get_date(url):\n",
    "    dates = re.findall(r\"\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d\", url)\n",
    "    return next(iter(dates), None)\n",
    "\n",
    "\n",
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) â†’ ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836bb990-9ce4-4001-a6cc-70dd52b6b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-07 15:05:58.911348][jupyter] BEGIN\n",
      "[2024-04-07 15:05:58.911467][jupyter] loading prepared data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "log(\"BEGIN\")\n",
    "log(\"loading prepared data...\")\n",
    "with open(\"../data/prepared.pkl\", \"rb\") as fp:\n",
    "    prepared = pkl.load(fp)\n",
    "vocabulary = prepared[\"vocabulary\"]\n",
    "texts = prepared[\"texts\"]\n",
    "contexts = prepared[\"contexts\"]\n",
    "test_texts = prepared[\"test_texts\"]\n",
    "y_train = prepared[\"y_train\"]\n",
    "y_test = prepared[\"y_test\"]\n",
    "text_train = prepared[\"texts_train\"]\n",
    "text_test = prepared[\"texts_test\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\").eval().cuda()\n",
    "batches = batched(text_train, 16)\n",
    "n_batches = len([b for b in batches])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4793d79-a0ef-4a59-b171-0a3ddd882505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-07 15:09:15.326316][jupyter][train] starting loop over n_batches=3960\n",
      "[2024-04-07 15:09:15.332760][jupyter][train][0/3960] input_ids.shape=torch.Size([16, 426]) attention_mask.shape=torch.Size([16, 426])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "batches = batched(text_train, 16)\n",
    "log(f\"starting loop over {n_batches=}\", [\"train\"])\n",
    "for i, batch in enumerate(batches):\n",
    "    tokens = tokenizer(\n",
    "        batch,\n",
    "        return_tensors='pt', \n",
    "        max_length=512, \n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        pad_to_max_length=True\n",
    "    )\n",
    "    input_ids=tokens[\"input_ids\"].cuda()\n",
    "    attention_mask=tokens[\"attention_mask\"].cuda()\n",
    "    log(\n",
    "        f\"{input_ids.shape=} {attention_mask.shape=}\", \n",
    "        [\"train\", f\"{i}/{n_batches}\"]\n",
    "    )\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    out_batch = out.hidden_states[-1].mean(1).detach().cpu().numpy()\n",
    "    outputs.append(out_batch)\n",
    "    break\n",
    "    # del batch, tokens,input_ids, attention_mask, out, out_batch\n",
    "    # log(f\"{i=}\", [\"train\", f\"{i}/{n_batches}\"])\n",
    "    # if 3 < i:\n",
    "    #     break\n",
    "output = np.vstack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3bd2946-559b-423b-b96f-816a8128ddf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 426, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7026d733-6b2e-4d2f-9ee7-d530bd8c0e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f83067-d201-4a4b-ace3-7dc057e3b07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(426, 768), (437, 768), (512, 768), (512, 768), (512, 768)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a798e1-c56d-492b-80f0-297dd7fff365",
   "metadata": {},
   "outputs": [],
   "source": [
    "out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
