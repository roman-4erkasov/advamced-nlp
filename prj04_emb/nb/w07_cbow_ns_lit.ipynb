{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d30767-e77f-4f69-b2b9-422602995949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import torch.nn as nn\n",
    "from types import NoneType\n",
    "from itertools import cycle\n",
    "import torch.optim as optim\n",
    "from utils import get_next_batch\n",
    "from vocabulary import Vocabulary\n",
    "from pytorch_lightning import Trainer\n",
    "from typing import Union, Mapping, Any\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "DEBUG = False\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "# FNAME, _ = os.path.splitext(os.path.basename(__file__))\n",
    "\n",
    "FNAME = \"w06\"\n",
    "\n",
    "def log(msg: str):\n",
    "    now = datetime.datetime.now()\n",
    "    dttm = now.strftime(format=\"%Y-%m-%d %H:%M:%S.%f\") \n",
    "    print(f\"[{dttm}] {msg}\")\n",
    "\n",
    "\n",
    "def get_samples(tokenized_texts, window_size, texts_count):\n",
    "    for text_num, tokens in enumerate(tokenized_texts):\n",
    "        if texts_count and text_num >= texts_count:\n",
    "            break\n",
    "        for i in range(len(tokens)):\n",
    "            central_word = torch.LongTensor(\n",
    "                [vocabulary.get_index(tokens[i])]\n",
    "            )\n",
    "            context = torch.LongTensor(\n",
    "                [\n",
    "                    vocabulary.get_index(tokens[i + delta])\n",
    "                    for delta in range(-window_size, window_size + 1)\n",
    "                    if 0 <= (i + delta) < len(tokens)\n",
    "                ]\n",
    "            ) \n",
    "            if 2*window_size == context.shape[0]:\n",
    "                yield central_word, context\n",
    "\n",
    "\n",
    "def get_samples_cycle(tokenized_texts, window_size, texts_count):\n",
    "    while True:\n",
    "        for sample in get_samples(tokenized_texts, window_size, texts_count):\n",
    "            yield sample\n",
    "\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, vocabulary, window_size=2, texts_count=100000):\n",
    "        self.samples = list(get_samples(tokenized_texts, window_size, texts_count))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "\n",
    "\n",
    "class Word2VecIterableDataset(IterableDataset):\n",
    "    def __init__(self, tokenized_texts, vocabulary, window_size=2, texts_count=None):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.vocabulary = vocabulary\n",
    "        self.window_size = window_size\n",
    "        self.texts_count = texts_count\n",
    "\n",
    "    def __iter__(self):\n",
    "        return get_samples_cycle(self.tokenized_texts, self.window_size, self.texts_count)\n",
    "\n",
    "\n",
    "class SkipGramModel(LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, sample_sz=4):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sample_sz = sample_sz\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.train_outputs = []\n",
    "        self.val_outputs = []\n",
    "        self.test_outputs = []\n",
    "    \n",
    "    def forward(self, centrals, contexts):\n",
    "        batch_sz = centrals.shape[0]\n",
    "        projections = self.embeddings.forward(contexts).sum(axis=1)\n",
    "        logits = self.out_layer.forward(projections)\n",
    "        centrals_ohe = torch.nn.functional.one_hot(centrals, self.vocab_size)\n",
    "        # out_loss = torch.mul(logits, centrals_ohe.squeeze()).sigmoid().log().squeeze().sum(1)\n",
    "        out_loss = torch.bmm(centrals_ohe.float(), logits.unsqueeze(2)).sum(1).sigmoid().log().squeeze()\n",
    "\n",
    "        # print(f\"{contexts.shape}\")\n",
    "        \n",
    "        noise_inp = torch.multinomial(\n",
    "            torch.ones(self.vocab_size),\n",
    "            batch_sz*self.sample_sz,\n",
    "            replacement=True\n",
    "        ).view(batch_sz, self.sample_sz)\n",
    "        noise_proj = self.embeddings.forward(noise_inp).sum(axis=1)\n",
    "        noise_logits = self.out_layer.forward(noise_proj)\n",
    "        noise_loss = torch.mul(noise_logits.neg(), centrals_ohe.squeeze()).sum(1).sigmoid().log().squeeze()\n",
    "        # noise_loss = torch.bmm(centrals_ohe.float(), noise_logits.neg().unsqueeze(2)).sigmoid().log().squeeze().sum(1)\n",
    "\n",
    "        return -(out_loss + noise_loss).mean()\n",
    "        # return -out_loss.mean()\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        result = self(*batch)\n",
    "        self.log(\"loss\", result)\n",
    "        return {'loss': result}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        result = self(*batch)\n",
    "        self.log(\"val_loss\", result)  \n",
    "        return {'val_loss': result}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        result = self(*batch)\n",
    "        self.log(\"test_loss\", result)\n",
    "        return {'test_loss': self(*batch)}\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self,\n",
    "        outputs: Union[torch.Tensor, Mapping[str, Any], NoneType],\n",
    "        batch: Any,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int = 0,\n",
    "    ) -> None:\n",
    "        self.train_outputs.append(outputs)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = self.train_outputs\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'loss': avg_loss}\n",
    "        self.log(\"train_loss_epoch\", avg_loss, on_step=False, on_epoch=True)\n",
    "        return {'train_loss_epoch': avg_loss, 'progress_bar': tensorboard_logs}\n",
    "    \n",
    "    def on_validation_batch_end(\n",
    "        self,\n",
    "        outputs: Union[torch.Tensor, Mapping[str, Any], NoneType],\n",
    "        batch: Any,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int = 0,\n",
    "    ) -> None:\n",
    "        self.val_outputs.append(outputs)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.val_outputs\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        self.log(\"val_loss_epoch\", avg_loss, on_step=False, on_epoch=True)\n",
    "        return {'val_loss_epoch': avg_loss, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "    def on_test_batch_end(\n",
    "        self,\n",
    "        outputs: Union[torch.Tensor, Mapping[str, Any], NoneType],\n",
    "        batch: Any,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int = 0,\n",
    "    ) -> None:\n",
    "        self.test_outputs.append(outputs)\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        outputs = self.test_outputs \n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'test_loss': avg_loss}\n",
    "        self.log(\"test_loss_epoch\", avg_loss, on_step=False, on_epoch=True)\n",
    "        return {'test_loss_epoch': avg_loss, 'progress_bar': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return [optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62a27ef-59a2-4a22-b5e7-f771f7768b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-02 06:03:48.949080] BEGIN\n",
      "[2024-04-02 06:03:48.949170] loading prepared data...\n",
      "[2024-04-02 06:05:22.737118] data loaded\n"
     ]
    }
   ],
   "source": [
    "log(\"BEGIN\")\n",
    "log(\"loading prepared data...\")\n",
    "with open(\"../data/prepared.pkl\", \"rb\") as fp:\n",
    "    prepared = pkl.load(fp)\n",
    "vocabulary = prepared[\"vocabulary\"]\n",
    "texts = prepared[\"texts\"]\n",
    "contexts = prepared[\"contexts\"]\n",
    "test_texts = prepared[\"test_texts\"]\n",
    "del prepared\n",
    "log(\"data loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103689d5-39a8-4c7b-aa5e-5350f120fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/fatuus/advanced-nlp/prj04_emb/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-02 06:05:35.326770] training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatuus/advanced-nlp/prj04_emb/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/fatuus/advanced-nlp/prj04_emb/nb/ckpt exists and is not empty.\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | embeddings | Embedding        | 9.1 M \n",
      "1 | out_layer  | Linear           | 9.2 M \n",
      "2 | loss       | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "18.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.3 M    Total params\n",
      "73.179    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                      | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatuus/advanced-nlp/prj04_emb/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatuus/advanced-nlp/prj04_emb/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|â–                                               | 146/40000 [00:31<2:22:38,  4.66it/s, v_num=62]"
     ]
    }
   ],
   "source": [
    "random.shuffle(texts)\n",
    "train_data = Word2VecIterableDataset(texts, vocabulary)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "random.shuffle(test_texts)\n",
    "val_data = Word2VecIterableDataset(test_texts, vocabulary)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "model = SkipGramModel(vocabulary.size)\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.0,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "ckpt_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"ckpt\",\n",
    "    filename=f\"{FNAME}-{{epoch}}-{{val_loss:.2f}}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    "    save_last=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=EPOCHS,\n",
    "    callbacks=[early_stop_callback, ckpt_callback],\n",
    "    limit_train_batches=2 if DEBUG else 40000,\n",
    "    limit_val_batches=2 if DEBUG else 500,\n",
    "    val_check_interval=1 if DEBUG else 2000,\n",
    "    # enable_progress_bar=False,\n",
    ")\n",
    "log(\"training...\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "log(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f10d14-149c-4fa0-a2d5-503f2cbbfef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f3e7c-2059-41df-b2a4-31fbb91ff20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebb138-c3b9-4f17-8488-1af1d4fb419f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cc32d-e758-4194-bd5d-e654736f4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in get_samples(texts, window_size=2, texts_count=7):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87333c54-573c-404c-904e-55d3a00f4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66eace2-e911-4ad5-b65a-d3d71cdf61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.one_hot(\n",
    "    torch.LongTensor([1,2,3]),\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384bc6f-6fb0-41bc-affd-01f58925e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(\n",
    "    torch.ones(vocabulary.size),\n",
    "    ,\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0f8fb-a149-4946-b54a-1a1bf08642e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor(\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [2,2,2]\n",
    "    ]\n",
    ")\n",
    "b = torch.LongTensor(\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [2,2,2]\n",
    "    ]\n",
    ")\n",
    "# torch.mul(a,b).sum(1)\n",
    "torch.bmm(a.unsqueeze(1), b.unsqueeze(2)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b45245-25ea-45d1-bb29-9d40a4768362",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(a,b).sum(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb2c3e-51fe-426e-a6a2-a02df1b97924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
